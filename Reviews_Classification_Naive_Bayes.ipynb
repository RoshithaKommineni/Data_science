{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kommineni651/Data_science/blob/main/Reviews_Classification_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fo1FgmaSwZ7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"Hamspam.csv\",encoding = \"ISO-8859-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YafE6QWiSwaD"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACyDv4IOSwaF"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR8QkWpBSwaG"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TB0HV4lSwaH"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca_Y24AtSwaJ"
      },
      "outputs": [],
      "source": [
        "import re #regular expression\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub(\"[0-9\" \"]+\",\" \",text)\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    return text\n",
        "\n",
        "clean = lambda x: clean_text(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBEfXMxNSwaK"
      },
      "outputs": [],
      "source": [
        "data['text'] = data.text.apply(clean)\n",
        "data.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0J-mT4ASwaL"
      },
      "outputs": [],
      "source": [
        "#Word frequency\n",
        "freq = pd.Series(' '.join(data['text']).split()).value_counts()[:20] # for top 20\n",
        "freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw2RO1J9SwaM"
      },
      "outputs": [],
      "source": [
        "#removing stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "551YwuTbSwaO"
      },
      "outputs": [],
      "source": [
        "#word frequency after removal of stopwords\n",
        "freq_Sw = pd.Series(' '.join(data['text']).split()).value_counts()[:20] # for top 20\n",
        "freq_Sw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-LvRvPISwaO"
      },
      "outputs": [],
      "source": [
        "# count vectoriser tells the frequency of a word.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "vectorizer = CountVectorizer(min_df = 1, max_df = 0.9)\n",
        "X = vectorizer.fit_transform(data[\"text\"])\n",
        "word_freq_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'occurrences':np.asarray(X.sum(axis=0)).ravel().tolist()})\n",
        "word_freq_df['frequency'] = word_freq_df['occurrences']/np.sum(word_freq_df['occurrences'])\n",
        "#print(word_freq_df.sort('occurrences',ascending = False).head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hosn6rLYSwaP"
      },
      "outputs": [],
      "source": [
        "word_freq_df.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPbjnJxCSwaP"
      },
      "outputs": [],
      "source": [
        "#TFIDF - Term frequency inverse Document Frequencyt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 0.5, smooth_idf=True) #keep top 1000 words\n",
        "doc_vec = vectorizer.fit_transform(data[\"text\"])\n",
        "names_features = vectorizer.get_feature_names()\n",
        "dense = doc_vec.todense()\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns = names_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DfrWSTjSwaQ"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnReRxduSwaR"
      },
      "source": [
        "# N-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcyA3ggASwaR"
      },
      "outputs": [],
      "source": [
        "#Bi-gram\n",
        "def get_top_n2_words(corpus, n=None):\n",
        "    vec1 = CountVectorizer(ngram_range=(2,2),  #for tri-gram, put ngram_range=(3,3)\n",
        "            max_features=2000).fit(corpus)\n",
        "    bag_of_words = vec1.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
        "                  vec1.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                reverse=True)\n",
        "    return words_freq[:n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh9aMvwuSwaS"
      },
      "outputs": [],
      "source": [
        "top2_words = get_top_n2_words(data[\"text\"], n=200) #top 200\n",
        "top2_df = pd.DataFrame(top2_words)\n",
        "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
        "top2_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Wff_KSLSwaS"
      },
      "outputs": [],
      "source": [
        "#Bi-gram plot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "top20_bigram = top2_df.iloc[0:20,:]\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "plot=sns.barplot(x=top20_bigram[\"Bi-gram\"],y=top20_bigram[\"Freq\"])\n",
        "plot.set_xticklabels(rotation=45,labels = top20_bigram[\"Bi-gram\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDFSzt62SwaT"
      },
      "outputs": [],
      "source": [
        "#Tri-gram\n",
        "def get_top_n3_words(corpus, n=None):\n",
        "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
        "           max_features=2000).fit(corpus)\n",
        "    bag_of_words = vec1.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
        "                  vec1.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
        "                reverse=True)\n",
        "    return words_freq[:n]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFKDEjDvSwaU"
      },
      "outputs": [],
      "source": [
        "top3_words = get_top_n3_words(data[\"text\"], n=200)\n",
        "top3_df = pd.DataFrame(top3_words)\n",
        "top3_df.columns=[\"Tri-gram\", \"Freq\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XJ3JispSwaV"
      },
      "outputs": [],
      "source": [
        "top3_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3Kpu1ifSwaV"
      },
      "outputs": [],
      "source": [
        "#Tri-gram plot\n",
        "import seaborn as sns\n",
        "top20_trigram = top3_df.iloc[0:20,:]\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "plot=sns.barplot(x=top20_trigram[\"Tri-gram\"],y=top20_trigram[\"Freq\"])\n",
        "plot.set_xticklabels(rotation=45,labels = top20_trigram[\"Tri-gram\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulj3V_TTSwaW"
      },
      "source": [
        "# WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6eaDXPJSwaW"
      },
      "outputs": [],
      "source": [
        "string_Total = \" \".join(data[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs8CnUjBSwaX"
      },
      "outputs": [],
      "source": [
        "#wordcloud for entire corpus\n",
        "from wordcloud import WordCloud\n",
        "wordcloud_stw = WordCloud(\n",
        "                background_color= 'black',\n",
        "                width = 1800,\n",
        "                height = 1500\n",
        "                ).generate(string_Total)\n",
        "plt.imshow(wordcloud_stw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aha0zcwiSwaX"
      },
      "source": [
        "# Applying naive bayes for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeXyfN76SwaX"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPMjh4TzSwaY"
      },
      "outputs": [],
      "source": [
        "def split_into_words(i):\n",
        "    return (i.split(\" \"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG7R4otlSwaY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "email_train,email_test = train_test_split(data,test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmryhXfRSwaZ"
      },
      "outputs": [],
      "source": [
        "email_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwFO77_SSwaZ"
      },
      "outputs": [],
      "source": [
        "# Preparing email texts into word count matrix format \n",
        "emails_bow = CountVectorizer(analyzer=split_into_words).fit(data.text)\n",
        "\n",
        "# [\"mailing\",\"body\",\"texting\"]\n",
        "# [\"mailing\",\"awesome\",\"good\"]\n",
        "\n",
        "# [\"mailing\",\"body\",\"texting\",\"good\",\"awesome\"]\n",
        "\n",
        "\n",
        "\n",
        "#        \"mailing\" \"body\" \"texting\" \"good\" \"awesome\"\n",
        "#  0          1        1       1        0       0\n",
        " \n",
        "#  1          1        0        0       1       1    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKzdJTmmSwaa"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So2dal4QSwaa"
      },
      "outputs": [],
      "source": [
        "# For all messages\n",
        "all_emails_matrix = emails_bow.transform(data.text)\n",
        "all_emails_matrix.shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF4epzTdSwab"
      },
      "outputs": [],
      "source": [
        "# For training messages\n",
        "train_emails_matrix = emails_bow.transform(email_train.text)\n",
        "train_emails_matrix.shape # (3891,8175)\n",
        "\n",
        "# For testing messages\n",
        "test_emails_matrix = emails_bow.transform(email_test.text)\n",
        "test_emails_matrix.shape # (1668,8175)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bAiMKzVSwab"
      },
      "outputs": [],
      "source": [
        "####### Without TFIDF matrices ########################\n",
        "# Preparing a naive bayes model on training data set \n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB as MB\n",
        "from sklearn.naive_bayes import GaussianNB as GB\n",
        "\n",
        "# Multinomial Naive Bayes\n",
        "classifier_mb = MB()\n",
        "classifier_mb.fit(train_emails_matrix,email_train.type)\n",
        "train_pred_m = classifier_mb.predict(train_emails_matrix)\n",
        "accuracy_train_m = np.mean(train_pred_m==email_train.type) # 98%\n",
        "\n",
        "test_pred_m = classifier_mb.predict(test_emails_matrix)\n",
        "accuracy_test_m = np.mean(test_pred_m==email_test.type) # 96%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KQN095kSwac"
      },
      "outputs": [],
      "source": [
        "# Gaussian Naive Bayes \n",
        "classifier_gb = GB()\n",
        "classifier_gb.fit(train_emails_matrix.toarray(),email_train.type.values) # we need to convert tfidf into array format which is compatible for gaussian naive bayes\n",
        "train_pred_g = classifier_gb.predict(train_emails_matrix.toarray())\n",
        "accuracy_train_g = np.mean(train_pred_g==email_train.type) # 95%\n",
        "\n",
        "test_pred_g = classifier_gb.predict(test_emails_matrix.toarray())\n",
        "accuracy_test_g = np.mean(test_pred_g==email_test.type) # 8%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr3lnYhNSwac"
      },
      "source": [
        "# Using TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUjM5-xRSwad"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Learning Term weighting and normalizing on entire emails\n",
        "tfidf_transformer = TfidfTransformer().fit(all_emails_matrix)\n",
        "\n",
        "# Preparing TFIDF for train emails\n",
        "train_tfidf = tfidf_transformer.transform(train_emails_matrix)\n",
        "\n",
        "train_tfidf.shape # (3891, 6661)\n",
        "\n",
        "# Preparing TFIDF for test emails\n",
        "test_tfidf = tfidf_transformer.transform(test_emails_matrix)\n",
        "\n",
        "test_tfidf.shape #  (1668, 6661)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGIeHRtdSwad"
      },
      "outputs": [],
      "source": [
        "# Preparing a naive bayes model on training data set \n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB as MB\n",
        "from sklearn.naive_bayes import GaussianNB as GB\n",
        "\n",
        "# Multinomial Naive Bayes\n",
        "classifier_mb = MB()\n",
        "classifier_mb.fit(train_tfidf,email_train.type)\n",
        "train_pred_m = classifier_mb.predict(train_tfidf)\n",
        "accuracy_train_m = np.mean(train_pred_m==email_train.type) # 96%\n",
        "\n",
        "test_pred_m = classifier_mb.predict(test_tfidf)\n",
        "accuracy_test_m = np.mean(test_pred_m==email_test.type) # 96%\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-uakktJSwae"
      },
      "outputs": [],
      "source": [
        "accuracy_train_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_DxgEGOSwae"
      },
      "outputs": [],
      "source": [
        "# Gaussian Naive Bayes \n",
        "classifier_gb = GB()\n",
        "classifier_gb.fit(train_tfidf.toarray(),email_train.type.values) # we need to convert tfidf into array format which is compatible for gaussian naive bayes\n",
        "train_pred_g = classifier_gb.predict(train_tfidf.toarray())\n",
        "accuracy_train_g = np.mean(train_pred_g==email_train.type) # 95%\n",
        "test_pred_g = classifier_gb.predict(test_tfidf.toarray())\n",
        "accuracy_test_g = np.mean(test_pred_g==email_test.type) # 88%\n",
        "\n",
        "# inplace of tfidf we can also use train_emails_matrix and test_emails_matrix instead of term inverse document frequency matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVYz9S1kSwae"
      },
      "outputs": [],
      "source": [
        "accuracy_test_g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zRRqz9cSwaf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Reviews_Classification_Naive Bayes.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}